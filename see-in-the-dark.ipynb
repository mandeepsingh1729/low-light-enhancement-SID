{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11358396,"sourceType":"datasetVersion","datasetId":7108673},{"sourceId":11358445,"sourceType":"datasetVersion","datasetId":7108717}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport numpy as np\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split  # Added random_split here\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_dir = \"/kaggle/input/low-ass\"\nhigh_dir = \"/kaggle/input/high-ass\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:20.998492Z","iopub.execute_input":"2025-04-12T05:58:20.998756Z","iopub.status.idle":"2025-04-12T05:58:21.002391Z","shell.execute_reply.started":"2025-04-12T05:58:20.998737Z","shell.execute_reply":"2025-04-12T05:58:21.001741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================\n# 1. Dataset Preparation\n# ====================================================\nclass LowLightDataset(Dataset):\n    def __init__(self, low_dir, high_dir, transform=None):\n        self.low_dir = low_dir\n        self.high_dir = high_dir\n        self.low_images = sorted(os.listdir(low_dir))\n        self.high_images = sorted(os.listdir(high_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.low_images)\n\n    def __getitem__(self, idx):\n        low_img_path = os.path.join(self.low_dir, self.low_images[idx])\n        high_img_path = os.path.join(self.high_dir, self.high_images[idx])\n        \n        low_img = Image.open(low_img_path).convert('RGB')\n        high_img = Image.open(high_img_path).convert('RGB')\n        \n        if self.transform:\n            low_img = self.transform(low_img)\n            high_img = self.transform(high_img)\n        \n        return low_img, high_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:21.409023Z","iopub.execute_input":"2025-04-12T05:58:21.409274Z","iopub.status.idle":"2025-04-12T05:58:21.414566Z","shell.execute_reply.started":"2025-04-12T05:58:21.409255Z","shell.execute_reply":"2025-04-12T05:58:21.413852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================\n# 2. Model Architecture \n# ====================================================\nclass DenoisingAutoencoder(nn.Module):\n    def __init__(self):\n        super(DenoisingAutoencoder, self).__init__()\n        \n        # Encoder\n        self.conv1_1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        \n        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n        \n        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n        \n        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        \n        # Decoder\n        self.upv6 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.conv6_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv6_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        \n        self.upv7 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv7_1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv7_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        \n        self.upv8 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.conv8_1 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n        self.conv8_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.upv9 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.conv9_1 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n        self.conv9_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        \n        # Output\n        self.conv10_1 = nn.Conv2d(32, 3, kernel_size=1, stride=1)\n    \n    def forward(self, x):\n        # Encoder\n        conv1 = self.lrelu(self.conv1_1(x))\n        conv1 = self.lrelu(self.conv1_2(conv1))\n        pool1 = self.pool1(conv1)\n        \n        conv2 = self.lrelu(self.conv2_1(pool1))\n        conv2 = self.lrelu(self.conv2_2(conv2))\n        pool2 = self.pool1(conv2)\n        \n        conv3 = self.lrelu(self.conv3_1(pool2))\n        conv3 = self.lrelu(self.conv3_2(conv3))\n        pool3 = self.pool1(conv3)\n        \n        conv4 = self.lrelu(self.conv4_1(pool3))\n        conv4 = self.lrelu(self.conv4_2(conv4))\n        pool4 = self.pool1(conv4)\n        \n        conv5 = self.lrelu(self.conv5_1(pool4))\n        conv5 = self.lrelu(self.conv5_2(conv5))\n        \n        # Decoder with skip connections\n        up6 = self.upv6(conv5)\n        up6 = torch.cat([up6, conv4], dim=1)\n        conv6 = self.lrelu(self.conv6_1(up6))\n        conv6 = self.lrelu(self.conv6_2(conv6))\n        \n        up7 = self.upv7(conv6)\n        up7 = torch.cat([up7, conv3], dim=1)\n        conv7 = self.lrelu(self.conv7_1(up7))\n        conv7 = self.lrelu(self.conv7_2(conv7))\n        \n        up8 = self.upv8(conv7)\n        up8 = torch.cat([up8, conv2], dim=1)\n        conv8 = self.lrelu(self.conv8_1(up8))\n        conv8 = self.lrelu(self.conv8_2(conv8))\n        \n        up9 = self.upv9(conv8)\n        up9 = torch.cat([up9, conv1], dim=1)\n        conv9 = self.lrelu(self.conv9_1(up9))\n        conv9 = self.lrelu(self.conv9_2(conv9))\n        \n        # Output\n        conv10 = self.conv10_1(conv9)\n        out = torch.sigmoid(conv10)  # Normalize to [0, 1]\n        return out\n\n    def lrelu(self, x):\n        return torch.max(0.2 * x, x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:21.866097Z","iopub.execute_input":"2025-04-12T05:58:21.866322Z","iopub.status.idle":"2025-04-12T05:58:21.880016Z","shell.execute_reply.started":"2025-04-12T05:58:21.866307Z","shell.execute_reply":"2025-04-12T05:58:21.879333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================\n# 3. Training Setup\n# ====================================================\n# Hyperparameters\nBATCH_SIZE = 4\nEPOCHS = 50\nLR = 0.0001\n\n# Transformations\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),  # Original paper uses 512x512\n    transforms.ToTensor(),\n])\n\n# Full dataset\nfull_dataset = LowLightDataset(\"/kaggle/input/low-ass\", \"/kaggle/input/high-ass\", transform=transform)\n\n# Split (Option 1)\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Initialize model, loss, optimizer\nmodel = DenoisingAutoencoder().to(device)\ncriterion = nn.L1Loss()  # MAE loss as in the paper\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:22.352602Z","iopub.execute_input":"2025-04-12T05:58:22.353135Z","iopub.status.idle":"2025-04-12T05:58:22.438834Z","shell.execute_reply.started":"2025-04-12T05:58:22.353105Z","shell.execute_reply":"2025-04-12T05:58:22.438331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================\n# 4. Training Loop\n# ====================================================\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n    best_loss = float('inf')\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        \n        # Training phase\n        for low, high in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            low, high = low.to(device), high.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(low)\n            loss = criterion(outputs, high)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for low, high in val_loader:\n                low, high = low.to(device), high.to(device)\n                outputs = model(low)\n                val_loss += criterion(outputs, high).item()\n        \n        # Print statistics\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n        \n        # Save best model\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:22.680175Z","iopub.execute_input":"2025-04-12T05:58:22.680419Z","iopub.status.idle":"2025-04-12T05:58:22.686359Z","shell.execute_reply.started":"2025-04-12T05:58:22.680401Z","shell.execute_reply":"2025-04-12T05:58:22.685688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start training\nmodel = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_metrics(model, val_loader):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    with torch.no_grad():\n        for low, high in tqdm(val_loader, desc=\"Evaluating\"):\n            low, high = low.to(device), high.to(device)\n            outputs = model(low).cpu().numpy()\n            high = high.cpu().numpy()\n            for i in range(outputs.shape[0]):\n                pred_img = np.clip(outputs[i].transpose(1, 2, 0), 0, 1)\n                true_img = np.clip(high[i].transpose(1, 2, 0), 0, 1)\n                total_psnr += psnr(true_img, pred_img, data_range=1.0)\n                total_ssim += ssim(true_img, pred_img, data_range=1.0, channel_axis=2)\n    \n    avg_psnr = total_psnr / len(val_loader.dataset)\n    avg_ssim = total_ssim / len(val_loader.dataset)\n    print(f\"\\n📊 Evaluation Results — PSNR: {avg_psnr:.2f} dB | SSIM: {avg_ssim:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_metrics(model, val_loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:58:38.407333Z","iopub.execute_input":"2025-04-12T05:58:38.407605Z","iopub.status.idle":"2025-04-12T06:08:44.174835Z","shell.execute_reply.started":"2025-04-12T05:58:38.407584Z","shell.execute_reply":"2025-04-12T06:08:44.173648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================================================\n# 5. Inference and Save Enhanced Images\n# ====================================================\ndef enhance_and_save(model, input_dir, output_dir, transform):\n    os.makedirs(output_dir, exist_ok=True)\n    model.eval()\n    \n    for img_name in tqdm(os.listdir(input_dir)):\n        img_path = os.path.join(input_dir, img_name)\n        img = Image.open(img_path).convert('RGB')\n        img_tensor = transform(img).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            enhanced_tensor = model(img_tensor)\n        \n        enhanced_img = transforms.ToPILImage()(enhanced_tensor.squeeze().cpu())\n        enhanced_img.save(os.path.join(output_dir, img_name))\n\n# Load best model\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.to(device)\n\n# Enhance test images\nenhance_and_save(model, low_dir, \"output/enhanced/\", transform)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}